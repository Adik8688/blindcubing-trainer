{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11064088",
   "metadata": {},
   "source": [
    "## Functions trash bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703e4f63",
   "metadata": {},
   "source": [
    "### Modifying file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ef9953e8-7244-4195-9c5a-e97415ea398d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os.path import exists\n",
    "\n",
    "def get_data(file_path):\n",
    "    if not exists(file_path):\n",
    "        return {}\n",
    "    \n",
    "    with open(file_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce562325",
   "metadata": {},
   "source": [
    "### Filtering data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d4c2292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(**attributes):\n",
    "    data = get_data()\n",
    "    result = dict()\n",
    "\n",
    "    for k, v in data.items():\n",
    "        if not any(v[k1] not in v1 for k1, v1 in attributes.items()):\n",
    "            result[k] = v\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a060a6e3",
   "metadata": {},
   "source": [
    "### Updating algs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "575739b6-074f-4915-9345-9aaf1e8a6bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "VALID_CHARS = \" UDFBRLMESudfbrlw'/:,2xyz\"\n",
    "\n",
    "def excel_to_dict_of_df(filename):\n",
    "    my_dict = pd.read_excel(filename, header=None, sheet_name=None)\n",
    "    my_dict = {k: df.replace(np.nan,'',regex=True) for k, df in my_dict.items()}\n",
    "    return my_dict\n",
    "\n",
    "\n",
    "def df_to_alg_list(df):\n",
    "    buffer = df.iloc[0][0]\n",
    "    if not buffer or df.shape[0] < 2 or df.shape[1] < 2 or not df.iloc[0][1]:\n",
    "        return None\n",
    "\n",
    "    piece_type = buffer_to_type(buffer)\n",
    "    result = []\n",
    "\n",
    "    # table\n",
    "    if df.iloc[1][0] == df.iloc[0][1]:\n",
    "        for i in range(1, df.shape[1]):\n",
    "            for j in range(1, df.shape[0]):\n",
    "                if df.iloc[i][j]:\n",
    "                    alg = clean_alg_entry(df.iloc[i][j])\n",
    "                    key = \";\".join([df.iloc[0][0], df.iloc[0][j], df.iloc[i][0], alg])\n",
    "                    result.append(key)\n",
    "        return {piece_type: result}\n",
    "    \n",
    "    if not (df.shape[1] == 4 or df.shape[1] == 5):\n",
    "        return None\n",
    "    # list\n",
    "    if df.shape[1] == 4:\n",
    "        row = 1\n",
    "        while df.iloc[row][0]:\n",
    "            alg = clean_alg_entry(df.iloc[row][3])\n",
    "            key = \";\".join([df.iloc[row][0], df.iloc[row][1], df.iloc[row][2], alg])\n",
    "            result.append(key)\n",
    "            row += 1\n",
    "        return {piece_type: result}\n",
    "    \n",
    "    # parity\n",
    "    row = 1\n",
    "    while df.iloc[row][0]:\n",
    "        alg = clean_alg_entry(df.iloc[row][4])\n",
    "        key = \";\".join([df.iloc[row][0], df.iloc[row][1], df.iloc[row][2], df.iloc[row][3], alg])\n",
    "        result.append(key)\n",
    "        row += 1\n",
    "    return {'parity': result}\n",
    "\n",
    "\n",
    "def clean_alg_entry(alg):\n",
    "    # filtering out invalid chars\n",
    "    alg = ''.join([i for i in alg if i in VALID_CHARS])\n",
    "\n",
    "    # cleaning multiple spaces \n",
    "    alg = ' '.join(alg.split())\n",
    "\n",
    "    # cleaning whitespaces before , : '\n",
    "    i = 0\n",
    "    while i < len(alg):\n",
    "        if alg[i] in \"':,\" and i > 0 and alg[i - 1] == \" \":\n",
    "            alg = alg[:i-1] + alg[i:]\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    return alg\n",
    "\n",
    "\n",
    "def only_latest_algs(data):\n",
    "    return {k: v for k, v in data.items() if v['latest_alg']}\n",
    "\n",
    "\n",
    "def keys_with_different_algs(data, key):\n",
    "    return [k for k in data if key.split(';')[:-1] == k.split(';')[:-1]]\n",
    "\n",
    "\n",
    "def is_key_in_dict(data, key):\n",
    "    return key in data\n",
    "\n",
    "\n",
    "def new_record_from_key(key):\n",
    "    key = key.split(';')\n",
    "    record = {\n",
    "        'buffer': key[0],\n",
    "        'first_target': key[1],\n",
    "        'second_target': key[2],\n",
    "        'results': [],\n",
    "        'latest': True\n",
    "    }\n",
    "\n",
    "    if len(key) == 4:\n",
    "        record['alg'] = key[3]\n",
    "        return record\n",
    "    \n",
    "    record['third_target'] = key[3]\n",
    "    record['alg'] = key[4]\n",
    "    return record\n",
    "    \n",
    "\n",
    "\n",
    "def update_algs(excel_path):\n",
    "    sheets = excel_to_dict_of_df(excel_path)\n",
    "    algs = dict()\n",
    "    for k, df in sheets.items():\n",
    "        algs_list = df_to_alg_list(df)\n",
    "\n",
    "        if algs_list is None:\n",
    "            continue\n",
    "        \n",
    "        print(k)\n",
    "\n",
    "        for piece_type, alg_list in algs_list.items():\n",
    "            algs[piece_type] = algs.get(piece_type, []) + alg_list\n",
    "        \n",
    "    for piece_type, algs_list in algs.items():\n",
    "        filepath = Path().absolute().parent / 'json' / f'{piece_type}.json'\n",
    "        data = get_data(str(filepath))\n",
    "        \n",
    "        for key in algs_list:\n",
    "            existing_algs = keys_with_different_algs(data, key)\n",
    "\n",
    "            for k in existing_algs:\n",
    "                data[k]['latest'] = False\n",
    "\n",
    "            if key in data:\n",
    "                data[key]['latest'] = True\n",
    "            else:\n",
    "                data[key] = new_record_from_key(key)\n",
    "\n",
    "        save_data(data, str(filepath))\n",
    "\n",
    "def buffer_to_type(buffer):\n",
    "    if len(buffer) < 2 or len(buffer) > 3:\n",
    "        return 'error'\n",
    "    \n",
    "    if buffer[0].islower():\n",
    "        return 'midges' + '_' + buffer.lower()\n",
    "    \n",
    "    if len(buffer) == 2 and buffer[1].islower():\n",
    "        return 'tcenters' + '_' + buffer.lower()\n",
    "    \n",
    "    if len(buffer) == 2:\n",
    "        return 'edges' + '_' + buffer.lower()\n",
    "    \n",
    "    if buffer[1].islower():\n",
    "        return 'xcenters' + '_' + buffer.lower()\n",
    "    \n",
    "    if buffer[2].islower():\n",
    "        return 'wings' + '_' + buffer.lower()\n",
    "    \n",
    "    return 'corners' + '_' + buffer.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b8e142",
   "metadata": {},
   "source": [
    "### Updating memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c835d988-f8a3-43d3-89a7-8a1818388a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def buffer_to_type(buffer, suffix = False):\n",
    "        if len(buffer) < 2 or len(buffer) > 3:\n",
    "            return \"error\"\n",
    "        \n",
    "        piece_type = ''\n",
    "\n",
    "        if buffer[0].islower():\n",
    "            piece_type = \"midges\"\n",
    "\n",
    "        elif len(buffer) == 2 and buffer[1].islower():\n",
    "            piece_type = \"tcenters\"\n",
    "\n",
    "        elif len(buffer) == 2:\n",
    "            piece_type = \"edges\"\n",
    "        elif buffer[1].islower():\n",
    "            piece_type = \"xcenters\"\n",
    "\n",
    "        elif buffer[2].islower():\n",
    "            piece_type = \"wings\"\n",
    "        \n",
    "        else:\n",
    "            piece_type = \"corners\"\n",
    "\n",
    "        if suffix:\n",
    "            piece_type = f'{piece_type}_{buffer}'\n",
    "        \n",
    "        return piece_type\n",
    "    \n",
    "\n",
    "def df_to_words_dict(df):\n",
    "    if df.shape[0] < 2 or df.shape[1] < 2 or not df.iloc[0][1]:\n",
    "        return None\n",
    "\n",
    "    piece_type = buffer_to_type(df.iloc[0][1])\n",
    "\n",
    "    result = dict()\n",
    "    if df.iloc[1][0] == df.iloc[0][1]:\n",
    "        for i in range(1, df.shape[1]):\n",
    "            for j in range(1, df.shape[0]):\n",
    "                if df.iloc[i][j]:\n",
    "                    key = f'{df.iloc[0][j]};{df.iloc[i][0]}'\n",
    "                    result[key] = df.iloc[i][j]\n",
    "        return {piece_type: result}\n",
    "\n",
    "\n",
    "    row = 1\n",
    "    while df.iloc[row][0]:\n",
    "        key = f'{df.iloc[row][0]};{df.iloc[row][1]}'\n",
    "        result[key] = df.iloc[row][2]\n",
    "        row += 1\n",
    "    return {piece_type: result}\n",
    "\n",
    "import os\n",
    "def update_words(filepath):\n",
    "    words = excel_to_dict_of_df(filepath)\n",
    "    words_dict = dict()\n",
    "    for df in words.values():\n",
    "        \n",
    "        words_grouped_by_type = df_to_words_dict(df)\n",
    "        \n",
    "        if words_grouped_by_type is None:\n",
    "            continue\n",
    "        \n",
    "        for piece_type, words_grouped_by_case in words_grouped_by_type.items():\n",
    "            if piece_type in words_dict:\n",
    "                words_dict[piece_type].update(words_grouped_by_case)\n",
    "            else:\n",
    "                words_dict[piece_type] = words_grouped_by_case\n",
    "    \n",
    "\n",
    "    for piece_type, words in words_dict.items():\n",
    "        path_to_jsons = Path().absolute().parent / \"json\"\n",
    "\n",
    "        jsons = []\n",
    "\n",
    "        for filename in os.listdir(path_to_jsons):\n",
    "            if filename.startswith(piece_type):\n",
    "                jsons.append(filename)\n",
    "\n",
    "        print(jsons)\n",
    "        for filename in jsons:\n",
    "            data = get_data(path_to_jsons / filename)\n",
    "\n",
    "            for k, v in data.items():\n",
    "                try:\n",
    "                    targets = f\"{v['first_target']};{v['second_target']}\"\n",
    "                    data[k]['word'] = words[targets]\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            save_data(data, path_to_jsons / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "48f1be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edges_db.json', 'edges_df.json', 'edges_dl.json', 'edges_dr.json', 'edges_fl.json', 'edges_fr.json', 'edges_ub.json', 'edges_uf.json', 'edges_ul.json', 'edges_ur.json']\n"
     ]
    }
   ],
   "source": [
    "update_words('memo.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c7ac17",
   "metadata": {},
   "source": [
    "### Updating LPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce0dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lps_dict(grid):\n",
    "    return {i[0]: i[1] for i in grid}\n",
    "\n",
    "def update_LPs():\n",
    "    lps = excel_to_dict_of_df('LP.csv')\n",
    "    lps_dict = get_lps_dict(lps)\n",
    "\n",
    "    for k, v in data.items():\n",
    "        try:\n",
    "            targets = [v['first_target'], v['second_target']]\n",
    "            data[k]['lp'] = ''.join([lps_dict[i] for i in targets])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dcd47327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "def get_data(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return {}\n",
    "\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_data(data, file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "\n",
    "def buffer_to_type(buffer, suffix = False):\n",
    "        if len(buffer) < 2 or len(buffer) > 3:\n",
    "            return \"error\"\n",
    "        \n",
    "        piece_type = ''\n",
    "\n",
    "        if buffer[0].islower():\n",
    "            piece_type = \"midges\"\n",
    "\n",
    "        elif len(buffer) == 2 and buffer[1].islower():\n",
    "            piece_type = \"tcenters\"\n",
    "\n",
    "        elif len(buffer) == 2:\n",
    "            piece_type = \"edges\"\n",
    "        elif buffer[1].islower():\n",
    "            piece_type = \"xcenters\"\n",
    "\n",
    "        elif buffer[2].islower():\n",
    "            piece_type = \"wings\"\n",
    "        \n",
    "        else:\n",
    "            piece_type = \"corners\"\n",
    "\n",
    "        if suffix:\n",
    "            piece_type = f'{piece_type}_{buffer}'\n",
    "        \n",
    "        return piece_type\n",
    "\n",
    "def excel_to_dict_of_dfs(filepath):\n",
    "        my_dict = pd.read_excel(filepath, header=None, sheet_name=None)\n",
    "        my_dict = {k: df.replace(np.nan, \"\", regex=True) for k, df in my_dict.items()}\n",
    "        return my_dict\n",
    "\n",
    "def df_to_lps_dict(df):\n",
    "        if df.shape[0] < 2 or df.shape[1] < 2 or not df.iloc[0][1]:\n",
    "            return None\n",
    "\n",
    "        piece_type = buffer_to_type(df.iloc[1][0])\n",
    "        result = dict()\n",
    "        for i in range(df.shape[0]):\n",
    "            result[df.iloc[i][0]] = df.iloc[i][1]\n",
    "        return {piece_type: result}\n",
    "\n",
    "\n",
    "def update_lps(filepath):\n",
    "    lps = excel_to_dict_of_dfs(filepath)\n",
    "    lps_dict = dict()\n",
    "    for df in lps.values():\n",
    "        \n",
    "        lps_grouped_by_type = df_to_lps_dict(df)\n",
    "        \n",
    "        if lps_grouped_by_type is None:\n",
    "            continue\n",
    "        print(lps_grouped_by_type)\n",
    "        for piece_type, words_grouped_by_case in lps_grouped_by_type.items():\n",
    "            if piece_type in lps_dict:\n",
    "                lps_dict[piece_type].update(words_grouped_by_case)\n",
    "            else:\n",
    "                lps_dict[piece_type] = words_grouped_by_case\n",
    "    \n",
    "\n",
    "    for piece_type, lps in lps_dict.items():\n",
    "        path_to_jsons = Path().absolute().parent / \"json\"\n",
    "\n",
    "        jsons = []\n",
    "\n",
    "        for filename in os.listdir(path_to_jsons):\n",
    "            if filename.startswith(piece_type):\n",
    "                jsons.append(filename)\n",
    "\n",
    "        for filename in jsons:\n",
    "            data = get_data(path_to_jsons / filename)\n",
    "\n",
    "            for k, v in data.items():\n",
    "                try:\n",
    "                    targets = [v['first_target'], v['second_target']]\n",
    "                    data[k]['lp'] = ''.join([lps[i] for i in targets])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            \n",
    "            save_data(data, path_to_jsons / filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1cfa5c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'edges': {'UB': 'A', 'UL': 'B', 'UR': 'C'}}\n"
     ]
    }
   ],
   "source": [
    "update_lps('lps.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51815d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56a8d3c1",
   "metadata": {},
   "source": [
    "### Running game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b8b952b-ffa7-4944-b319-cef66c117497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "def get_n_random_keys(data, n):\n",
    "    data = {k: v for k, v in data.items() if v['latest']}\n",
    "    keys = list(data)\n",
    "    shuffle(keys)\n",
    "    return keys[:n]    \n",
    "\n",
    "\n",
    "def measure_time():\n",
    "    start = datetime.now()\n",
    "    input()\n",
    "    end = datetime.now()\n",
    "    delta = (end - start).total_seconds()\n",
    "    return round(delta, 2)\n",
    "\n",
    "\n",
    "def display_record(alg):\n",
    "    if 'word' in alg:\n",
    "        print(alg['word'])\n",
    "    elif 'lp' in alg:\n",
    "        print(alg['lp'])\n",
    "    else:\n",
    "        print(f\"{alg['first_target']} {alg['second_target']}\")\n",
    "\n",
    "\n",
    "def get_response():\n",
    "    print()\n",
    "    response = input('Type Y to save results')\n",
    "    return response == 'Y'\n",
    "\n",
    "\n",
    "def add_entries(data, results):\n",
    "    for k, v in results.items():\n",
    "        data[k]['results'].append(v)\n",
    "\n",
    "    save_data(data)\n",
    "\n",
    "def run_game(n):\n",
    "    results_dict = dict()\n",
    "    data = get_data()\n",
    "    algs = get_n_random_keys(data, n)\n",
    "    for alg in algs:\n",
    "        display_record(data[alg])\n",
    "        exec_time = measure_time()\n",
    "        results_dict[alg] = exec_time\n",
    "\n",
    "    print('Results to be saved:')\n",
    "    for k, v in results_dict.items():\n",
    "        print(' '.join(k.split(';')[1:3]), v)\n",
    "\n",
    "    if get_response():\n",
    "        add_entries(data, results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e5034",
   "metadata": {},
   "source": [
    "### El śmietniko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "10713559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UF UR UB\n",
      "R2 U: S, R2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = {\n",
    "    \"latest\": [False]\n",
    "}\n",
    "\n",
    "filtered_data = filter_data(**query)\n",
    "for k, v in filtered_data.items():\n",
    "    print(v['buffer'], v['first_target'], v['second_target'])\n",
    "    print(v['alg'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23b194e7-04aa-4615-9c14-dffd666be44f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"R: R E' R', U'\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = \"[Ra    :  R  (1E ' 6  R    '&&   , %9 U       ']\"\n",
    "clean_alg_entry(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5355d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(update_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0f644ea7-a99a-486e-a5ec-8f06c41b5e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arbuz\n",
      "Bat\n",
      "Cable\n",
      "Acid\n",
      "Baca\n",
      "Results to be saved:\n",
      "UB UL 0.71\n",
      "UL UB 0.92\n",
      "UR UL 1.29\n",
      "UB UR 2.81\n",
      "UL UR 3.06\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_game(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "61b22b1b-a3e3-4f65-a42f-a58fe6f98234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel('data.xlsx', sheet_name = None) # can also index sheet by name or fetch all sheets\n",
    "df_uf = df['UF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c32ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UB'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uf.iloc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c8a0026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UF'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_uf.keys()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7451af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('algs.csv', sep=';', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9c541eb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UF</td>\n",
       "      <td>UB</td>\n",
       "      <td>UL</td>\n",
       "      <td>UR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U' M2 U: U2, M</td>\n",
       "      <td>R2 U' S' U2 S U' R2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UL</td>\n",
       "      <td>U' M2 U: M, U2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M2 U: M, U2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UR</td>\n",
       "      <td>R2 U': S, R2</td>\n",
       "      <td>M2 U': M, U2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0               1               2                    3\n",
       "0  UF              UB              UL                   UR\n",
       "1  UB             NaN  U' M2 U: U2, M  R2 U' S' U2 S U' R2\n",
       "2  UL  U' M2 U: M, U2             NaN          M2 U: M, U2\n",
       "3  UR    R2 U': S, R2    M2 U': M, U2                  NaN"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ebf3f5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"U' M2 U: M, U2\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.iloc[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "40513f66",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df \u001b[39m=\u001b[39m get_sheet_content(\u001b[39m'\u001b[39m\u001b[39malgs.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df_to_alg_list(df)\n",
      "Cell \u001b[1;32mIn[55], line 14\u001b[0m, in \u001b[0;36mdf_to_alg_list\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mlen\u001b[39m(df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m])):\n\u001b[0;32m     13\u001b[0m     \u001b[39mif\u001b[39;00m df\u001b[39m.\u001b[39miloc[i][j]:\n\u001b[1;32m---> 14\u001b[0m         alg \u001b[39m=\u001b[39m clean_alg_entry(df\u001b[39m.\u001b[39;49miloc[i][j])\n\u001b[0;32m     15\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m], df\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][j], df\u001b[39m.\u001b[39miloc[i][\u001b[39m0\u001b[39m], alg])\n\u001b[0;32m     16\u001b[0m         result\u001b[39m.\u001b[39mappend(key)\n",
      "Cell \u001b[1;32mIn[55], line 22\u001b[0m, in \u001b[0;36mclean_alg_entry\u001b[1;34m(alg)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclean_alg_entry\u001b[39m(alg):\n\u001b[0;32m     21\u001b[0m     \u001b[39m# filtering out invalid chars\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     alg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m alg \u001b[39mif\u001b[39;00m i \u001b[39min\u001b[39;00m VALID_CHARS])\n\u001b[0;32m     24\u001b[0m     \u001b[39m# cleaning multiple spaces \u001b[39;00m\n\u001b[0;32m     25\u001b[0m     alg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(alg\u001b[39m.\u001b[39msplit())\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae771d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[\"R2 U'\"], ['R2', ' S']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = \"R2 U': R2, S\"\n",
    "t1 = t1.split(':')\n",
    "t1 = [i.strip().split(',') for i in t1]\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ea080ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5865a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Move:\n",
    "    def __init__(self, move):\n",
    "        self.move = move\n",
    "        self.set_base_move()\n",
    "        self.set_suffix(''.join([i for i in self.move if not i.isalpha()]))\n",
    "        self.set_axis()\n",
    "\n",
    "\n",
    "    def get_move(self):\n",
    "        return self.base_move + self.suffix\n",
    "\n",
    "    def set_base_move(self):\n",
    "        self.base_move = ''.join([i for i in self.move if i not in \"'2\"])\n",
    "\n",
    "    def get_base_move(self):\n",
    "        return self.base_move\n",
    "\n",
    "    def set_suffix(self, suff):\n",
    "        self.suffix = suff \n",
    "\n",
    "    def get_suffix(self):\n",
    "        return self.suffix\n",
    "    \n",
    "    def get_inv_suffix(self):\n",
    "        if self.suffix == \"'\":\n",
    "            return ''\n",
    "        if self.suffix == '':\n",
    "            return \"'\"\n",
    "        return self.suffix\n",
    "\n",
    "    def set_axis(self):\n",
    "        \n",
    "        def axis_check(moves):\n",
    "            return self.base_move[0] in moves\n",
    "        \n",
    "        if axis_check('UED'):\n",
    "            self.axis = 'y'\n",
    "        \n",
    "        elif axis_check('LMR'):\n",
    "            self.axis = 'x'\n",
    "\n",
    "        else:\n",
    "            self.axis = 'z'\n",
    "\n",
    "    def get_axis(self):\n",
    "        return self.axis\n",
    "\n",
    "    def inverse_move(self):\n",
    "        if self.suffix == \"'\":\n",
    "            self.suffix = ''\n",
    "\n",
    "        elif self.suffix == '':\n",
    "            self.suffix = \"'\"\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.base_move + self.suffix\n",
    "        \n",
    "\n",
    "class ComutatorAnalyzer:\n",
    "    def __init__(self, comm):\n",
    "        self.comm = comm\n",
    "        self.alg = self.expandComm()\n",
    "    \n",
    "    @staticmethod\n",
    "    def list_to_alg(alg, inv=False):\n",
    "        alg = [Move(i) for i in alg]\n",
    "\n",
    "        if inv:\n",
    "            alg = alg[::-1]\n",
    "            for i in alg:\n",
    "                i.inverse_move()\n",
    "\n",
    "        return alg\n",
    "\n",
    "    @staticmethod\n",
    "    def splitted_comm_to_alg(a, b, c=''):\n",
    "        base = ComutatorAnalyzer.list_to_alg(a) + ComutatorAnalyzer.list_to_alg(b) + ComutatorAnalyzer.list_to_alg(a, True) + ComutatorAnalyzer.list_to_alg(b, True)\n",
    "\n",
    "        if c:\n",
    "            return ComutatorAnalyzer.list_to_alg(c) + base + ComutatorAnalyzer.list_to_alg(c, True)\n",
    "        \n",
    "        return base\n",
    "\n",
    "    def split_comm(self):\n",
    "        split_by_sep = [i.strip() for i in re.split(':|,', self.comm)]\n",
    "        return [i.split() for i in split_by_sep]\n",
    "\n",
    "    @staticmethod\n",
    "    def reduce_moves(m1, m2):\n",
    "        if m1.get_axis() != m2.get_axis():\n",
    "            return [m1, m2]\n",
    "        \n",
    "        def suff_to_pos(suffix):\n",
    "            if suffix == '':\n",
    "                return 1\n",
    "            if suffix == '2':\n",
    "                return 2\n",
    "            return 3\n",
    "        \n",
    "        def suff_sum(s1, s2):\n",
    "            pos = suff_to_pos(s1) + suff_to_pos(s2)\n",
    "            return pos % 4\n",
    "\n",
    "        def pos_to_move(base_move, pos):\n",
    "            if pos == 0:\n",
    "                return \n",
    "            if pos == 1: \n",
    "                return Move(base_move)\n",
    "            if pos == 2:\n",
    "                return Move(base_move + '2')\n",
    "            return Move(base_move + \"'\")\n",
    "\n",
    "\n",
    "        if m1.get_base_move() == m2.get_base_move():\n",
    "\n",
    "            pos = suff_sum(m1.get_suffix(), m2.get_suffix())\n",
    "            \n",
    "            return [pos_to_move(m1.get_base_move(), pos)]\n",
    "        \n",
    "        if m1.get_axis() == 'x':\n",
    "            order = ['R', 'Rw', 'M', 'Lw', 'L']\n",
    "\n",
    "        elif m1.get_axis() == 'y':\n",
    "            order = ['U', 'Uw', 'E', 'Dw', 'D']\n",
    "        \n",
    "        else:\n",
    "            order = ['B', 'Bw', 'S', 'Fw', 'F']\n",
    "\n",
    "\n",
    "        if order.index(m1.get_base_move()) > order.index(m2.get_base_move()):\n",
    "            m1, m2 = m2, m1\n",
    "        \n",
    "        b1, b2 = m1.get_base_move(), m2.get_base_move()\n",
    "        s1, s2 = m1.get_suffix(), m2.get_suffix()\n",
    "\n",
    "        if b1 == order[0]:\n",
    "            if s1 == m2.get_inv_suffix():\n",
    "                if b2 == order[1]:\n",
    "                    return [Move(order[2] + m2.get_inv_suffix())]\n",
    "                if b2 == order[2]:\n",
    "                    return [Move(order[1] + s1)]\n",
    "            \n",
    "            if b2 == order[1]:\n",
    "                pos = suff_sum(s1, s2)\n",
    "                return [pos_to_move(b1, pos), Move(order[2] + m2.get_inv_suffix())]\n",
    "\n",
    "        if b1 == order[1] and b2 == order[2]:\n",
    "            if s1 == s2:\n",
    "                return [Move(order[0] + s1)]\n",
    "            \n",
    "            else:\n",
    "                pos = suff_sum(s1, s2)\n",
    "                return [Move(order[0] + s1), pos_to_move(b2, pos)]\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        if b1 == order[2] and b2 == order[3] and s1 == m2.get_inv_suffix():\n",
    "            return [Move(order[4] + m2.get_inv_suffix())]\n",
    "        \n",
    "        if b1 == order[2] and b2 == order[4] and s1 == s2:\n",
    "            return [Move(order[3] + s2)]\n",
    "        \n",
    "        if b1 == order[3] and b2 == order[4] and s1 == m2.get_inv_suffix():\n",
    "            return [Move(order[2] + s1)]\n",
    "        \n",
    "        return [m1, m2]\n",
    " \n",
    "\n",
    "    def expandComm(self):\n",
    "        splitted_comm = self.split_comm()\n",
    "\n",
    "        a, b = splitted_comm[-2], splitted_comm[-1]\n",
    "        c = splitted_comm[0] if splitted_comm[0] != splitted_comm[-2] else ''\n",
    "\n",
    "        alg = ComutatorAnalyzer.splitted_comm_to_alg(a, b, c)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(alg) - 1:\n",
    "            reduced_pair = ComutatorAnalyzer.reduce_moves(alg[i], alg[i + 1])\n",
    "\n",
    "            if reduced_pair != alg[i : i + 2]:\n",
    "                alg = alg[:i] + reduced_pair + alg[i + 2:]\n",
    "                print(alg)\n",
    "                i = 0\n",
    "\n",
    "            else:\n",
    "                i+=1\n",
    "\n",
    "\n",
    "        self.alg = alg\n",
    "\n",
    "    def get_alg_str(self):\n",
    "        return ' '.join([i.get_move() for i in self.alg])\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26d6fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def comm_to_alg(comm):\n",
    "    ca = ComutatorAnalyzer(comm)\n",
    "    ca.expandComm()\n",
    "    return ca.get_alg_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b325e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uw': L E' L', U2 -> Uw' L E' L' U2 L E L' U' E'\n"
     ]
    }
   ],
   "source": [
    "comms = [\n",
    "    \"Uw': L E' L', U2\",\n",
    "    \"Uw': L U' L', E2\",\n",
    "    # \"U' Rw': E, R U R'\",\n",
    "    # \"U' M' U': M, U2\",\n",
    "    # \"Uw': R' E R, U\"\n",
    "]\n",
    "\n",
    "for comm in comms:\n",
    "    print(f\"{comm} -> {comm_to_alg(comm)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "aad94b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[L2, Rw]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Move('Rw')\n",
    "b = Move('L2')\n",
    "sorted([a,b], key=lambda x: x.get_move())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e299835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[R2, U'], [R2], [S]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm = \"R2 U': R2, S\"\n",
    "split_by_sep = [i.strip() for i in re.split(':|,', comm)]\n",
    "[[Move(j) for j in i.split()] for i in split_by_sep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec65abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uw', 'U', 'E', 'Dw', 'D']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(['U', 'Uw', 'E', 'D', 'Dw'], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "deb6f168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d', 'a', 'b', 'c', 'e', 'f']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_find = 'd'\n",
    "a = ['a', 'b', 'c', 'd', 'e', 'f']\n",
    "i = a.index(to_find)\n",
    "b = [a[i]] + a[:i] + a[i+1:]\n",
    "b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63723774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.index('c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77b7d65a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c', 'd', 'e', 'f']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebae852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
